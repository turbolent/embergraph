import net.jini.jeri.BasicILFactory;
import net.jini.jeri.BasicJeriExporter;
import net.jini.jeri.tcp.TcpServerEndpoint;

import net.jini.discovery.LookupDiscovery;
import net.jini.core.discovery.LookupLocator;
import net.jini.core.entry.Entry;
import net.jini.lookup.entry.Name;
import net.jini.lookup.entry.Comment;
import net.jini.lookup.entry.Address;
import net.jini.lookup.entry.Location;
import net.jini.lookup.entry.ServiceInfo;
import net.jini.core.lookup.ServiceTemplate;

import java.io.File;

import org.embergraph.util.NV;
import org.embergraph.journal.BufferMode;
import org.embergraph.jini.lookup.entry.*;
import org.embergraph.service.IBigdataClient;
import org.embergraph.service.jini.*;
import org.embergraph.service.jini.lookup.DataServiceFilter;
import org.embergraph.service.jini.master.ServicesTemplate;
import org.embergraph.jini.start.config.*;
import org.embergraph.jini.util.ConfigMath;

import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.data.ACL;
import org.apache.zookeeper.data.Id;

// imports for various options.
import org.embergraph.btree.IndexMetadata;
import org.embergraph.btree.keys.KeyBuilder;
import org.embergraph.rdf.sail.BigdataSail;
import org.embergraph.rdf.spo.SPORelation;
import org.embergraph.rdf.spo.SPOKeyOrder;
import org.embergraph.rdf.lexicon.LexiconRelation;
import org.embergraph.rdf.lexicon.LexiconKeyOrder;
import org.embergraph.rawstore.Bytes;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeUnit.*;

/*
 * This is a sample configuration file for a bigdata federation.
 * 
 * Note: The original file is a template.  The template contains parameters
 * of the form @XXX@.  The values for those template parameters are specified
 * in the build.properties file when you use ant to install bigdata.
 * 
 * Note: This file uses the jini configuration mechanism.  The syntax
 * is a subset of Java.  The properties for each component are grouped
 * within the namespace for that component.
 *
 * See the net.jini.config.ConfigurationFile javadoc for more
 * information.
 */

/*
 * A namespace use for static entries referenced elsewhere in this
 * ConfigurationFile.
 */
bigdata {

    /**
     * The name for this federation.
     *
     * Note: This is used to form the [zroot] (root node in zookeeper
     * for the federation) and the [serviceDir] (path in the file
     * system for persistent state for the federation).
     *
     * Note: If you will be running more than one federation, then you
     * MUST use unicast discovery and specify the federation name in
     * the [groups].
     */
    static private fedname = "@FED@";

    /**
     * Where to put all the persistent state.
     */
    static private serviceDir = new File("@LAS@");

	/**
	 * Which JDK to use.
	 */
	static private javaHome = new File("@JAVA_HOME@");
	
    /**
     * A common point to set the Zookeeper client's requested
     * sessionTimeout and the jini lease timeout.  The default lease
     * renewal period for jini is 5 minutes while for zookeeper it is
     * more like 5 seconds.  This puts the two systems onto a similar
     * timeout period so that a disconnected client is more likely to
     * be noticed in roughly the same period of time for either
     * system.  A value larger than the zookeeper default helps to
     * prevent client disconnects under sustained heavy load.
     */
    // jini
    static private leaseTimeout = ConfigMath.m2ms(60);// 20s=20000; 5m=300000; 
    // zookeeper
    static private sessionTimeout = (int)ConfigMath.m2ms(10);// was 5m 20s=20000; 5m=300000; 

    /*
     * Example cluster configuration.
     *
     * Data services are load balanced.  Index partitions will be
     * moved around as necessary to ensure hosts running data
     * service(s) are neither under nor over utilized.  Data services
     * can be very resource intensive processes.  They heavily buffer
     * both reads and writes, and they use RAM to do so.  They also
     * support high concurrency and can use up to one thread per index
     * partition.  How many cores they will consume is very much a
     * function of the application.
     *
     * Zookeeper services use a quorum model.  Always allocate an odd
     * number.  3 gives you one failure.  5 gives you two failures.
     * Zookeeper will sync the disk almost continuously while it is
     * running.  It really deserves its own local disk.  Zookeeper
     * also runs in memory.  Since all operations are serialized, if
     * it starts swapping then performance will drop through the floor.
     *
     * Jini uses a peer model.  Each service registers with each
     * registrar that it discovers.  Each client listeners to each
     * registrar that it discovers.  The default jini core services
     * installation runs entirely in memory (no disk operations, at
     * least not for service registration). A second instance of the
     * jini core services provides a safety net.  If you are using
     * multicast then you can always add another instance.
     */

    /* One machine running NFS, the load balancer, the transaction
     * server, the metadata service, and a jini service.  The only
     * DISK intensive process is NFS.  This is also the machine on
     * which I am logged in and is the master for distributed jobs.
     */
    // 21 - 35 are online
    static private nfs = "192.168.1.10"; // blade2
    static private lbs = "192.168.1.10";
    static private txs = "192.168.1.10";
    static private mds = "192.168.1.10";
    // 2 jini servers (only one, since no console on the other machine)
    static private jini1 = "192.168.1.10"; //"192.168.6.21";
    //static private jini2 = "blade3"; //"192.168.6.23"; // doubled up with zoo2
    static private jini = new String[]{jini1};//,jini2};
    // 2 class servers : @todo support here and in small cluster setup.
    static private cls1 = "192.168.1.10";
    //static private cls2 = "192.168.6.23"; // doubled up with zoo2
    static private cls = new String[]{cls1};
    // 3 zookeeper machines (one instance per).
    static private zoo1 = "192.168.1.10";//"192.168.6.22"; // blade3
    //static private zoo2 = "blade4";//"192.168.6.23"; // blade4
    //static private zoo3 = "blade5";//"192.168.6.24"; // blade5
    static private zoo = new String[] {zoo1};
    // 14 client/data service machines (could be more than one instance per).
    static private cs1  = "192.168.1.11"; // blade6 // alt cs/ds
    static private cs2  = "192.168.1.12"; // blade9
    static private ds2  = "192.168.1.12"; // blade10
    static private ds3  = "192.168.1.13"; // blade11
    static private ds4  = "192.168.1.14"; // blade12
    static private ds5  = "192.168.1.15"; // blade13
    static private ds6  = "192.168.1.16"; // blade14
    static private ds7  = "192.168.1.17"; // blade15

    // client servers
    static private cs = new String[] {
    	cs1//, cs2
    };

    // The target #of client servers.
    static private maxClientServicePerHost = 1;
    static private clientServiceCount = 1;

    // data servers
    static private ds = new String[]{
	   ds2, 
	   ds3, 
	   ds4, 
	   ds5,
	   ds6, ds7
 	   };

    // The target #of masters.
    static private maxDataServicesPerHost = 1;
    static private dataServiceCount = 6;

    // Sets the initial and maximum journal extents.
    static private journalExtent = ConfigMath.multiply(200, Bytes.megabyte);

    /**
     * zookeeper configuration properties.
     */
     static private zkClientPort = 2181;

    /**
     * A String[] whose values are the group(s) to be used for discovery
     * (no default). Note that multicast discovery is always used if
     * LookupDiscovery.ALL_GROUPS (a <code>null</code>) is specified.
     */

    // one federation, multicast discovery.
    //static private groups = LookupDiscovery.ALL_GROUPS;

    // unicast discovery or multiple federations, MUST specify groups.
    static private groups = new String[]{bigdata.fedname};

    /**
     * One or more unicast URIs of the form <code>jini://host/</code>
     * or <code>jini://host:port/</code> (no default).
     *
     * This MAY be an empty array if you want to use multicast
     * discovery <strong>and</strong> you have specified the groups as
     * LookupDiscovery.ALL_GROUPS (a <code>null</code>).
     */
    static private locators = new LookupLocator[] {

	// runs jini on the localhost using unicast locators.
	//new LookupLocator("jini://localhost/")
	
	// runs jini on two hosts using unicast locators.
	new LookupLocator("jini://"+jini1),
	//new LookupLocator("jini://"+jini2),

    };

    /**
     * The policy file that will be used to start services.
     */
    private static policy = "@POLICY_FILE@";

    /**
     * log4j configuration file (applies to bigdata and zookeeper).
     *
     * Note: The value is URI!
     *
     * Note: You should aggregate all of the log output to a single
     * host.  For example, using the log4j SocketAppender and the
     * SimpleNodeServer.
     */
    log4j = "@LOG4J_CONFIG@";

    /**
     * java.util.logging configuration file (applies to jini as used
     * within bigdata).
     *
     * Note: The value is a file path!
     */
    logging = "@LOGGING_CONFIG@";

    /*
    private static host = ConfigUtil.getHostName();
    private static port = "8081";
    private static jskdl = " http://" + host + ":" + port + "/jsk-dl.jar";
    */

    /**
     * JVM argument may be used to enable the yourkit profiler agent on a
     * service.  Of course, yourkit must be installed at this location and
     * you must have a licensed copy of the yourkit UI running either on a
     * node of the cluster or on a machine routed to the cluster, e.g., via
     * an ssh tunnel.  The yourkit profiler uses ports in [10001:100010] by
     * default on each node. 
     *
     * See http://www.yourkit.com/docs/80/help/running_with_profiler.jsp
     *
     * See http://www.yourkit.com/docs/80/help/agent.jsp
     *
     * See http://www.yourkit.com/docs/80/help/additional_agent_options.jsp
     *
     * Note: Conditionally include ${profilerAgent} iff you want to enable
     * profiling for some service class.
     */

     // linux-64 with all profiling options initially disabled.
     profilerAgent="-agentpath:/nas/install/yjp-10.0.1/bin/linux-x86-64/libyjpagent.so=disableexceptiontelemetry,disablestacktelemetry";

}

/*
 * Service configuration defaults.  These can also be specified on a
 * per service-type basis.  When the property is an array type, the
 * value here is concatenated with the optional array value on the per
 * service-type configuration.  Otherwise it is used iff no value is
 * specified for the service-type configuration.
 */
org.embergraph.jini.start.config.ServiceConfiguration {

    /* 
     * Default java command line arguments that will be used for all
     * java-based services
     *
     * Note: [-Dcom.sun.jini.jeri.tcp.useNIO=true] enables NIO in
     * combination with the [exporter] configured below.
     */
    defaultJavaArgs = new String[]{
	"-server",
	"-ea",
	"-showversion",
	//"-Xmx2G",
     /* This is a workaround for a JVM bug which can result in a
      * lost wakeup.  This bug is fixed in JDK1.6.0_18.  However,
      * JDK1.6.0_18 has other problems which result in segfaults.
      *
      * http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6822370
      */
    "-XX:+UseMembar",
	"-Dcom.sun.jini.jeri.tcp.useNIO=@USE_NIO@",
	"-Djava.security.policy="+bigdata.policy,
	"-Djava.util.logging.config.file="+bigdata.logging,
	"-Dorg.embergraph.counters.linux.sysstat.path=@SYSSTAT_HOME@",
	// Note: no longer a valid property.
	//"-Djava.rmi.dgc.leaseValue=30000",
	//"-Dsun.rmi.dgc.checkInterval=15000",
	"-Dcom.sun.jini.jeri.dgc.leaseValue=30000",
	"-Dcom.sun.jini.jeri.dgc.checkInterval=15000",
	"-Dsun.rmi.transport.tcp.connectionPool=true",
	//bigdata.profilerAgent,
	/*
	 * Override the LRU buffer capacity.
	 *
	 * See org.embergraph.LRUNexus.Options for configuration info.  Note that if
	 * you disable the LRUNexus you will loose the leaf cache for the index
	 * segments, which is a big penalty.
	//"-Dorg.embergraph.LRUNexus.enabled=false",
	// option may be used to select the higher throughput impl.
	"-Dorg.embergraph.LRUNexus.class=org.embergraph.cache.HardReferenceGlobalLRURecyclerExplicitDeleteRequired",
	// option may be used to allocate more heap to the LRUNexus.
	"-Dorg.embergraph.LRUNexus.percentHeap=.2",
	"-Dorg.embergraph.LRUNexus.indexSegmentBuildPopulatesCache=true", // default true
	 */
    };

    /* Default path for service instances and their persistent
     * data. This may be overridden on a per service-type basis. 
     *
     * Note: For logical services that support failover, the concrete
     * service directory is assigned dynamically when a physical
     * service instance is created.
     */
    serviceDir = bigdata.serviceDir;

	// The JVM to use.
	javaHome = bigdata.javaHome;

    /* The bigdata services default logging configuration (a URI!)
     */
    log4j = bigdata.log4j;

    /*
     * Set up some default properties values that will be inherited
     * (copy by value) by all clients and services started using this
     * configuration file.
     */
    properties = new NV[] {

    /* 
     * Option may be set to report out all metrics which can be
     * collected to the LBS.
     */
    new NV(IBigdataClient.Options.REPORT_ALL,"@REPORT_ALL@"),

    /* 
     * Each JiniClient (and hence all bigdata services) can run an
     * httpd that will expose performance counters for the service and
     * the host on which it is running.  This property specifies the
     * port for that httpd service.  Valid values are port number,
     * zero (0) for a random open port, MINUS ONE (-1) to disable the
     * httpd service.
     */
    //new NV(IBigdataClient.Options.HTTPD_PORT, "-1"),

    /*
     * Option to disable collection of performance counters for the
     * host on which the client or service is running.
     *
     * Note: The load balancer relies on this information!
     */
    //new NV(IBigdataClient.Options.COLLECT_PLATFORM_STATISTICS,"false"),

    /* Option to disable collection of performance counters on the
     * queues used internally by the client or service.
     *
     * Note: The load balancer relies on this information!
     */
    //new NV(IBigdataClient.Options.COLLECT_QUEUE_STATISTICS,"false"),

    /* Option controls how many times a client request will be
     * reissued on receiving notice that an index partition locator is
     * stale.  Stale locators arise when an index partition is split,
     * moved, or joined.
     *
     * Note: This option needs to be larger if we are aggressively
     * driving journal overflows and index partitions splits during
     * the "young" phase of a data service or scale-out index since a
     * LOT of redirects will result.
     */
    new NV(IBigdataClient.Options.CLIENT_MAX_STALE_LOCATOR_RETRIES,"1000"),

    /* Option limits the amount of parallelism per operation requested against
     * a scale-out index view.  Each parallel request takes a Thread, so there
     * needs to be a limit on the fan out of requests or the virtual process
     * size will shoot up as stack is allocated for new threads. (You can also
     * adjust the size of the stack to compensate.)
     */
    new NV(IBigdataClient.Options.CLIENT_MAX_PARALLEL_TASKS_PER_REQUEST,"10"),

    };

}

/**
 * JoinManager options.
 *
 * Note: These options must be copied into the service.config (to
 * specify the service lease timeout) as well as used by the client
 * (which uses this file directly).
 */
net.jini.lookup.JoinManager {

    // The lease timeout for jini joins.
    maxLeaseDuration = bigdata.leaseTimeout;

}

/**
 * Jini service configuration.
 */
jini {

	/* This sets command line arguments for the ServiceStarter which
	 * is used to run the jini services.
     */
    args = new String[] {

        "-Xmx400m",
        "-Djava.security.policy="+bigdata.policy,
        "-Djava.util.logging.config.file="+bigdata.logging,

    };

    /**
     * The main jini configuration file.  This file contains a
     * NonActivatableServiceDescriptor[]. The elements of that array
     * describe how to start each of the jini services.
     */
    configFile = new File("@JINI_CONFIG@");

    /**
     * The #of instances to run.
     *
     * Note: A jini service instance may be started on a host if it is
     * declared in [locators].  If locators is empty, then you are
     * using multicast discovery.  In this case an instance may be
     * started on any host, unless [constraints] are imposed.  In any
     * case, no more than [serviceCount] jini services will be started
     * at any given time.  This is checked against the #of discovered
     * instances.
     */
    serviceCount = 1;

    /**
     * @see <a href="http://sourceforge.net/apps/trac/bigdata/ticket/183">trac 183</a>
     */
    timeout = 20000 ;
}

/**
 * Zookeeper server configuration.
 */
org.apache.zookeeper.server.quorum.QuorumPeerMain {

    /* Directory for zookeeper's persistent state.  The [id] will be
     * appended as another path component automatically to keep
     * instances separate.
     */
    dataDir = new File(bigdata.serviceDir,"zookeeper");

    /* Optional directory for the zookeeper log files.  The [id] will
     * be appended as another path component automatically to keep
     * instances separate.
     * 
     * Note: A dedicated local storage device is highly recommended
     * for the zookeeper transaction logs!
     */
    //dataLogDir=new File("/var/zookeeper-log");

    // required.
    clientPort=bigdata.zkClientPort;

    tickTime=2000;

    initLimit=5;

    syncLimit=2;

    /* A comma delimited list of the known zookeeper servers together
     * with their assigned "myid": {myid=host:port(:port)}+
     *
     * Note: You SHOULD specify the full list of servers that are
     * available to the federation. An instance of zookeeper will be
     * started automatically on each host running ServicesManager that
     * is present in the [servers] list IF no instance is found
     * running on that host at the specified [clientPort].
     * 
     * Note: zookeeper interprets NO entries as the localhost with
     * default peer and leader ports. This will work as long as the
     * localhost is already running zookeeper.  However, zookeeper
     * WILL NOT automatically start zookeeper if you do not specify
     * the [servers] property.  You can also explicitly specify
     * "localhost" as the hostname, but that only works for a single
     * machine.
     */
    // standalone
    //servers="1=localhost:2888:3888";
    // ensemble
    /**/
    servers =  "1="+bigdata.zoo1+":2888:3888"
//            + ",2="+bigdata.zoo2+":2888:3888"
//	    + ",3="+bigdata.zoo3+":2888:3888"
	    ;

    // This is all you need to run zookeeper, but you have to update dependencies by hand.
//    classpath = new String[] {
//    	"@LIB_DIR@/apache/zookeeper-3.2.1.jar",
//        "@LIB_DIR@/apache/log4j-1.2.15.jar"
//    };

    /* Optional command line arguments for the JVM used to execute
     * zookeeper.
     *
     * Note: swapping for zookeeper is especially bad since the
     * operations are serialized, so if anything hits then disk then
     * all operations in the queue will have that latency as well.
     * However, bigdata places a very light load on zookeeper so
     * a modest heap should be Ok. For example, I have observed a
     * process size of only 94m after 10h on a 15-node cluster.
     */
    args = new String[]{
	"-Xmx200m",
	/*
	 * Enable JXM remote management.
	 *
	"-Dcom.sun.management.jmxremote.port=9997",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
};

    // zookeeper server logging configuration (value is a URI!)
    log4j = bigdata.log4j;

}

/*
 * Zookeeper client configuration.
 */
org.apache.zookeeper.ZooKeeper {

    /* Root znode for the federation instance. */
    zroot = "/"+bigdata.fedname;

    /* A comma separated list of host:port pairs, where the port is
     * the CLIENT port for the zookeeper server instance.
     */
    // standalone.
    // servers = "localhost:2181";
    // ensemble
    servers =   bigdata.zoo1+":"+bigdata.zkClientPort // @TODO enable other instances.
//             + ","+bigdata.zoo2+":2181"
// 	    + ","+bigdata.zoo3+":2181"
	    ;

    /* Session timeout (optional). */
    sessionTimeout = bigdata.sessionTimeout;

    /* 
     * ACL for the zookeeper nodes created by the bigdata federation.
     *
     * Note: zookeeper ACLs are not transmitted over secure channels
     * and are placed into plain text Configuration files by the
     * ServicesManagerServer.
     */
    acl = new ACL[] {

	new ACL(ZooDefs.Perms.ALL, new Id("world", "anyone"))

    };

}

/*
 * Jini client configuration
 */
org.embergraph.service.jini.JiniClient {

    /* Default Entry[] for jini services.  Also used by the
     * ServicesManagerService as is.
     *
     * Note: A Name attribute will be added automatically using the
     * service type and the znode of the service instance.  That Name
     * will be canonical.  It is best if additional service names are
     * NOT specified as that might confuse somethings :-)
     *
     * Note: A Hostname attribute will be added dynamically.
     */
    entries = new Entry[] {
	// Purely informative.
	new Comment(bigdata.fedname),
    };

    groups = bigdata.groups;

    locators = bigdata.locators;

    // optional JiniClient properties.
    properties = new NV[] {
    
    /* 
     * Option may be set to report out all metrics which can be
     * collected to the LBS.
     */
    new NV(IBigdataClient.Options.REPORT_ALL,"@REPORT_ALL@"),
    
    };

    /*
     * Overrides for jini SERVICES (things which are started
     * automatically) BUT NOT CLIENTs (things which you start by hand
     * and which read this file directly).
     *
     * The difference here is whether or not a service.config file is
     * being generated.  When it is, the jiniOptions[] will be
     * included in how that service is invoked and will operate as
     * overrides for the parameters specified in the generated
     * service.config file.  However, normal clients directly consume
     * this config file rather than the generated one and therefore
     * you must either specify their overrides directly on the command
     * line when you start the client or specify them explicitly in
     * the appropriate component section within this configuration
     * file.
     *
     * In practice, this means that you must specify some parameters
     * both here and in the appropriate component configuration. E.g.,
     * see the component section for "net.jini.lookup.JoinManager"
     * elsewhere in this file.
     */
    jiniOptions = new String[] {

	// The lease timeout for jini joins.
	"net.jini.lookup.JoinManager.maxLeaseDuration="+bigdata.leaseTimeout,

    };

}

/**
 * Options for the bigdata services manager.
 */
org.embergraph.jini.start.ServicesManagerServer {

    /*
     * This object is used to export the service proxy.  The choice
     * here effects the protocol that will be used for communications
     * between the clients and the service.
     */
    exporter = new BasicJeriExporter(TcpServerEndpoint.getInstance(0),
                                     new BasicILFactory()); 

    /*                                          
     * The data directory and the file on which the serviceID will be
     * written.
     *
     * Note: These properties MUST be specified explicitly for the
     * ServicesManager since it uses this as its Configuration file.
     * For other services, it generates the Configuration file and
     * will generate this property as well.
     */

    serviceDir = new File(bigdata.serviceDir,"ServicesManager");

    serviceIdFile = new File(serviceDir,"service.id");
    
    /* The services that will be started.  For each service, there
     * must be a corresponding component defined within this
     * configuration file.  For each "ManagedServiceConfiguration", an
     * entry will be made in zookeeper and logical and physical
     * service instances will be managed automatically.  For unmanaged
     * services, such as jini and zookeeper itself, instances will be
     * started iff necessary by the services manager when it starts
     * up.
	 */
    services = new String[] {
	
      	"jini",
 		"org.apache.zookeeper.server.quorum.QuorumPeerMain",
  		"org.embergraph.service.jini.TransactionServer",
    	"org.embergraph.service.jini.MetadataServer",
    	"org.embergraph.service.jini.DataServer",
    	"org.embergraph.service.jini.LoadBalancerServer",
    	"org.embergraph.service.jini.ClientServer"
	
    };

    /*
     * Additional properties passed through to the JiniClient or the
     * service.
     *
     * Note: The services manager is used to collect statistics from the
     * OS for each host so we have performance counters for hosts which
     * are only running non-bigdata services, such as jini or zookeeper.
     */
    properties = new NV[]{

    };

    /* The services manager MUDT be run on every host so that it may
     * start both bigdata and non-bigdata services (jini, zookeeper).
     * This is also used to report per-host performance counters to
     * the load balancer for hosts that are not running bigdata 
     * services.
     */
    constraints = new IServiceConstraint[] {

    };

}

org.embergraph.service.jini.TransactionServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.txs)

    };

    args = new String[]{
	
	bigdata.profilerAgent,	
		// Does not need much RAM.
		"-Xmx200m"
	};
	
	properties = new NV[] {
	
    /* 
     * Option may be set to report out all metrics which can be
     * collected to the LBS.
     */
    new NV(IBigdataClient.Options.REPORT_ALL,"@REPORT_ALL@"),

	/* The #of milliseconds that the database will retain history no
	 * longer required to support the earliest active transaction.
	 *
	 * A value of ZERO means that only the last commit point will
	 * be retained.  The larger the value the more history will be
	 * retained.  You can use a really big number if you never want
	 * to release history and you have lots of disk space :-)
	 *
	 * Note: The most recent committed state of the database is
	 * NEVER released.
	 */
	new NV(TransactionServer.Options.MIN_RELEASE_AGE, "0"),

	};
	
}

org.embergraph.service.jini.MetadataServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),
	//new TXRunningConstraint(),

	new HostAllowConstraint(bigdata.mds),

    };

    args = new String[]{
	
		// Does not need much RAM.
		"-Xmx200m"
	
	};

    properties = new NV[]{

    /* 
     * Option may be set to report out all metrics which can be
     * collected to the LBS.
     */
    new NV(IBigdataClient.Options.REPORT_ALL,"@REPORT_ALL@"),

    /*
	 * The MDS does not support overflow at this time so
	 * overflow MUST be disabled for this service.
	 */
        new NV(MetadataServer.Options.OVERFLOW_ENABLED,"false")    

    };

}

org.embergraph.service.jini.DataServer {

    args = new String[]{
	/* 
	 * Grant lots of memory, but read on.
	 *
	 * Note: 32-bit JVMs have a 2G limit on the heap, but the practical limit
	 * is often much less - maybe 1400m.  64-bit JVMs can use much more RAM.
	 * However, the heap which you grant to java DOES NOT determine the total
	 * process heap.  I have seen 64-bit java processes using an additional
	 * 3-4GB of heap beyond what is specified here.  So, you need to consider
	 * the total RAM, subtract out enough for the other processes and the OS
	 * buffers, divide by the #of client/data services you plan to run on that
	 * host (generally 1-2) and then subtract out some more space for the JVM
	 * itself.  
	 *
	 * For example, if you have 32G RAM and a 64-bit JVM and plan to run two
	 * CS/DS on the host, I would recommend 10G for the Java heap.  You can
	 * expect to see Java grab another 4G per process over time.  That makes
	 * the per CS/DS heap 14G.  With two processes you have taken 28G leaving
	 * 4G for everything else.
	 *
	 * Here is another example: 4G RAM, 32-bit JVM, and 2 CS/DS per host.  I
	 * would stick to 800m for the Java heap.  You don't have a problem unless
	 * you see an OOM (OutOfMemoryException) or a process killed because GC is
	 * taking too much time.
	 *
	 * See http://www.ibm.com/developerworks/linux/library/j-nativememory-linux/index.html?ca=dgr-lnxw07Linux-JVM&S_TACT=105AGX59&S_CMP=grlnxw07
	 * See http://lwn.net/Articles/83588/
	 *
	 * Note: for linux, "sysctl -w vm.swappiness=0" will keep the RAM you do
	 * have for your applications!
	 *
	 * On win64 platforms, the kernal can wind up consuming all
     * available memory for IO heavy applications, resulting in heavy
     * swapping and poor performance.  The links below can help you to
     * understand this issue and include a utility to specify the size
     * of the file system cache.
	 *
     * http://www.microsoft.com/downloads/details.aspx?FamilyID=e24ade0a-5efe-43c8-b9c3-5d0ecb2f39af&displaylang=en
     * http://blogs.msdn.com/ntdebugging/archive/2009/02/06/microsoft-windows-dynamic-cache-service.aspx
	 */
	"-Xmx3G", // Note: out of 32 available!
	bigdata.profilerAgent,
	"-Xdebug","-Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=1046",
	/* Bigdata uses 1 buffer per index segment. Each buffer is
	 * between 1 and 3 MB, depending on how things are configured.
	 * For example: 1G/2.5MB ~ 400 shards.  Direct buffers are also
	 * used for the live journal and a few other things so this is
	 * just approximate.
	 */
	"-XX:MaxDirectMemorySize=2g",
	/* Pre-allocation of the DS heap is no longer recommended.
	 *
	 * See https://sourceforge.net/apps/trac/bigdata/ticket/157
	"-Xms9G", 
	 */
	/*
	 * FIXME This might not be required, so that should be tested.
	 * However, you don't want the JVM to just die if it is being
	 * heavily loaded by GC so this is probably a good idea
	 * anyway.
	 */
	"-XX:-UseGCOverheadLimit",
	/* Configure GC for higher throughput.  Together these options
	 * request parallel old generation collection using N threads.
	 * The application will be paused when this occurs, but GC will
	 * be faster.  Hence throughput will be higher.  However, be
	 * sure to use JDK 6u10+ (6676016 : ParallelOldGC leaks memory).
	 * 
	 * http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6676016
	 */
	"-XX:+UseParallelOldGC",
	//"-XX:ParallelGCThreads=4",
	/* Generates detailed logging on the JVM GC behavior.  The service
	 * will start in the configured service directory, so the log file
	 * will be in that directory as well.  The service directory is
	 * typically on local disk, so that is where you need to look for
	 * this file.
	"-XX:+PrintGCDetails",
	"-XX:+PrintGCTimeStamps",
	"-Xloggc:jvm_gc.log",
	 */
	/* Enable huge page utilization.  This is a VM and OS specific tweak. 
	 * For linux, you must configure the kernel for huge pages first. The
	 * flag should be specified for processes with large heaps when huge
	 * page support is available.   
	 *
	 * See http://andrigoss.blogspot.com/2008/02/jvm-performance-tuning.html,
	 * http://java.sun.com/javase/technologies/hotspot/largememory.jsp, and
	 * 'bigdatasetup'.
	 */
	//"-XX:+UseLargePages",
	/**/
	/*
	 * Enable JXM remote management for the data service.
	 *
	 * Note: This will not work if you have two data services on a host 
	 * because it will assign the same port to each service.  In order
	 * to work around that the argument would have to be specified by
	 * the service starter and then published in the Entry[] attributes.
	 *
	 * However, you can use ssh -X to open a tunnel with X
	 * forwarding and then run jconsole locally on the target host
	 * and bring up these data services without enabling remote
	 * JMX.
	 *
	"-Dcom.sun.management.jmxremote.port=9999",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
         /*
         * Override the size of the default pool of direct (native) byte
         * buffers.  This was done to ensure that the nodes region for
         * index segments remain fully buffered as the index partitions
         * approach their maximum size before a split.
         */
         "-Dorg.embergraph.io.DirectBufferPool.bufferCapacity="+
                ConfigMath.multiply(Bytes.kilobyte,1000),
     };

    serviceCount = bigdata.dataServiceCount;

    // restrict where the data services can run.
    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),
	//new TXRunningConstraint(),

	new HostAllowConstraint(bigdata.ds),

	new MaxDataServicesPerHostConstraint(bigdata.maxDataServicesPerHost),

    };

    /*
     * Note: the [dataDir] will be filled in when a new service
     * instance is created based on the [servicesDir], so don't set it
     * here yourself.
     */
    properties = new NV[]{

    /* 
     * Option may be set to report out all metrics which can be
     * collected to the LBS.
     */
    new NV(IBigdataClient.Options.REPORT_ALL,"@REPORT_ALL@"),

	new NV(DataServer.Options.BUFFER_MODE,
	    //""+org.embergraph.journal.BufferMode.Direct
	    ""+org.embergraph.journal.BufferMode.DiskWORM
	    ),

	/* Option disables synchronous overflow after N times and
	 * configures the offset bits for the journal for a scale-up
	 * configuration so we may use very large journals.
	 */
	//new NV(DataServer.Options.OVERFLOW_MAX_COUNT,"5"),
	//new NV(DataServer.Options.OFFSET_BITS,""+org.embergraph.rawstore.WormAddressManager.SCALE_UP_OFFSET_BITS),

	/* Synchronous overflow is triggered when the live journal is
	 * this full (the value is a percentage, expressed as a
	 * floating point number in [0:1]).
	 */
	//new NV(DataServer.Options.OVERFLOW_THRESHOLD,".9"),

	/* Override the initial and maximum extent so that they are more
	 * more suited to large data sets.  Overflow will be triggered as
	 * the size of the journal approaches the maximum extent.  The
	 * initial and maximum extent are configured up above.
	 */

	new NV(DataServer.Options.INITIAL_EXTENT, "" + bigdata.journalExtent),
	new NV(DataServer.Options.MAXIMUM_EXTENT, "" + bigdata.journalExtent),

	/* Specify the queue capacity for the write service (unisolated
	 * write operations).
	 * 
	 * 0 := SynchronousQueue.
	 * N := bounded queue of capacity N
	 * Integer.MAX_VALUE := unbounded queue. 
	 *
	 * Note: The corePoolSize will never increase for an unbounded
	 * queue so the value specified for maximumPoolSize will
	 * essentially be ignored in this case. 
	 *
	 * Note: A SynchronousQueue is a good choice here since it allows
	 * the #of threads to change in response to demand.  The pool
	 * size should be unbounded when using a SynchronousQueue.
	 */
	new NV(DataServer.Options.WRITE_SERVICE_QUEUE_CAPACITY,"0"), // synchronous queue.
	new NV(DataServer.Options.WRITE_SERVICE_CORE_POOL_SIZE,"5"),
	new NV(DataServer.Options.WRITE_SERVICE_MAXIMUM_POOL_SIZE,"100"),
	new NV(DataServer.Options.WRITE_SERVICE_PRESTART_ALL_CORE_THREADS,"false"),

	/*
	 * Options turns off overflow processing (debugging only).
	 * All writes will go onto the live journal, no index segments
	 * will be built, and indices will not be split, moved,
	 * joined, etc.
	 */
	//new NV(DataServer.Options.OVERFLOW_ENABLED,"false"),

	/* Maximum #of index partition moves per overflow.
	 */
	new NV(DataServer.Options.MAXIMUM_MOVES,"1"),

	/* Option controls how many index partitions may be moved onto
	 * any given target data service in a single overflow cycle
	 * and may be used to disable index partition moves (for
	 * debugging purposes).
	 */
	new NV(DataServer.Options.MAXIMUM_MOVES_PER_TARGET,"1"),

	/* The minimum CPU activity on a host before it will consider moving an
	* index partition to shed some load.
	*/
	new NV(DataServer.Options.MOVE_PERCENT_CPU_TIME_THRESHOLD,".85"),//was .7

	/* Option limits the #of index segments in a view before a
	 * compacting merge is forced.
	 */
	new NV(DataServer.Options.MAXIMUM_SEGMENTS_PER_VIEW,"5"), // default 6

	/* Option limits the #of optional merges that are performed in each
	 * overflow cycle.
	 */
	new NV(DataServer.Options.MAXIMUM_OPTIONAL_MERGES_PER_OVERFLOW,"1"),

	/* Option effects how much splits are emphasized for a young
	 * scale-out index.  If the index has fewer than this many
	 * partitions, then there will be a linear reduction in the
	 * target index partition size which will increase the likelyhood
	 * of an index split under heavy writes. This helps to distribute
	 * the index early in its life cycle.
	 */
	new NV(DataServer.Options.ACCELERATE_SPLIT_THRESHOLD,"20"),//20//50

	/* Options accelerates overflow for data services have fewer than
	 * the threshold #of bytes under management.  Acceleration is
	 * accomplished by reducing the maximum extent of the live journal
	 * linearly, but with a minimum of a 10M maximum extent.  When the
	 * maximum extent is reduced by this option, the initial and the
	 * maximum extent will always be set to the same value for that
	 * journal.
	 */
	new NV(DataServer.Options.ACCELERATE_OVERFLOW_THRESHOLD,
	    //"0"
	    //""+org.embergraph.rawstore.Bytes.gigabyte
    	    "2147483648" // 2G
	    ),

    // #of threads for index segment builds (default 3).
    new NV(DataServer.Options.BUILD_SERVICE_CORE_POOL_SIZE,"5"),

    // #of threads for compacting merges (default 1).
    new NV(DataServer.Options.MERGE_SERVICE_CORE_POOL_SIZE,"2"),

//	// Zero is full parallelism; otherwise #of threads in the pool.
//	new NV(DataServer.Options.OVERFLOW_TASKS_CONCURRENT,"20"),

	/* Use Long.MAX_VALUE to always run overflow processing to
	 * completion (until no more data remains on the old journal).
	 */
	new NV(DataServer.Options.OVERFLOW_TIMEOUT,""+Long.MAX_VALUE),

	new NV(DataServer.Options.OVERFLOW_CANCELLED_WHEN_JOURNAL_FULL,"false"),

    new NV(DataServer.Options.LIVE_INDEX_CACHE_CAPACITY,"10"), // was 60

    new NV(DataServer.Options.HISTORICAL_INDEX_CACHE_CAPACITY,"10"), // was 60

        /* The maximum #of clean indices that will be retained on the
         * hard reference queue (default 20).
         */
        new NV(DataServer.Options.INDEX_CACHE_CAPACITY,"10"), // was 50

        /* The timeout (ms) for unused index references before they are
         * cleared from the hard reference queue (default is 1m).
         * After this timeout the index reference is cleared from the
         * queue and the index will be closed unless a hard reference
         * exists to the index.
         */
       new NV(DataServer.Options.INDEX_CACHE_TIMEOUT,"20000"), // default 60s

        /* The maximum #of clean index segments that will be retained
         * on the hard reference queue (default 60).  Note that ALL
         * index segments are clean (they are read-only).
         */
        new NV(DataServer.Options.INDEX_SEGMENT_CACHE_CAPACITY,"20"), // was 100

        /* The timeout for unused index segment references before they
         * are cleared from the hard reference queue (default is 1m).
         * After this timeout the index segment reference is cleared
         * from the queue and the index segment will be closed unless
         * a hard reference exists to the index segment.
         */
//       new NV(DataServer.Options.INDEX_SEGMENT_CACHE_TIMEOUT,"60000000"), // 10m vs 1m

        /* The #of store files (journals and index segment stores)
         * whose hard references will be maintained on a queue.  The
         * value should be slightly more than the index segment cache
         * capacity since some journals also used by the views, but
         * same journals are shared by all views so adding 3 is plenty..
         */
        new NV(DataServer.Options.STORE_CACHE_CAPACITY,"23"),// was 110

//       new NV(DataServer.Options.STORE_CACHE_TIMEOUT,"1200000"),//20m vs 1m. 
	
    };

}

/**
 * Configuration options for the containers used to distribute application tasks
 * across a federation.
 *
 * @todo There should be a means to tag certain client servers for one purpose
 * or another.  This could be handled by subclassing, but it really should be
 * declarative.
 */
org.embergraph.service.jini.ClientServer {

    args = new String[]{
	/* 
	 * Grant lots of memory, but read on.
	 *
	 * Note: 32-bit JVMs have a 2G limit on the heap, but the practical limit
	 * is often much less - maybe 1400m.  64-bit JVMs can use much more RAM.
	 * However, the heap which you grant to java DOES NOT determine the total
	 * process heap.  I have seen 64-bit java processes using an additional
	 * 3-4GB of heap beyond what is specified here.  So, you need to consider
	 * the total RAM, subtract out enough for the other processes and the OS
	 * buffers, divide by the #of client/data services you plan to run on that
	 * host (generally 1-2) and then subtract out some more space for the JVM
	 * itself.  
	 *
	 * For example, if you have 32G RAM and a 64-bit JVM and plan to run two
	 * CS/DS on the host, I would recommend 10G for the Java heap.  You can
	 * expect to see Java grab another 4G per process over time.  That makes
	 * the per CS/DS heap 14G.  With two processes you have taken 28G leaving
	 * 4G for everything else.
	 *
	 * Here is another example: 4G RAM, 32-bit JVM, and 2 CS/DS per host.  I
	 * would stick to 800m for the Java heap.  You don't have a problem unless
	 * you see an OOM (OutOfMemoryException) or a process killed because GC is
	 * taking too much time.
	 *
	 * See http://www.ibm.com/developerworks/linux/library/j-nativememory-linux/index.html?ca=dgr-lnxw07Linux-JVM&S_TACT=105AGX59&S_CMP=grlnxw07
	 *
	 * Note: for linux, "sysctl -w vm.swappiness=0" will keep the RAM you do
	 * have for your applications!
	 */
	"-Xmx6G", // Note: out of 32 available!
	/*
	 * Note: This might not be required, so that should be tested.
	 * However, you don't want the JVM to just die if it is being
	 * heavily loaded by GC so this is probably a good idea
	 * anyway.
	 */
	"-XX:-UseGCOverheadLimit",
	/* Configure GC for higher throughput.  Together these options
	 * request parallel old generation collection using N threads.
	 * The application will be paused when this occurs, but GC will
	 * be faster.  Hence throughput will be higher.  However, be
	 * sure to use JDK 6u10+ (6676016 : ParallelOldGC leaks memory).
	 * 
	 * http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6676016
	 */
	"-XX:+UseParallelOldGC",
	//"-XX:ParallelGCThreads=4",
	/* Generates detailed logging on the JVM GC behavior.  The service
	 * will start in the configured service directory, so the log file
	 * will be in that directory as well.  The service directory is
	 * typically on local disk, so that is where you need to look for
	 * this file.
	"-XX:+PrintGCDetails",
	"-XX:+PrintGCTimeStamps",
	"-Xloggc:jvm_gc.log",
	 */
	/* Enable huge page utilization.  This is a VM and OS specific tweak. 
	 * For linux, you must configure the kernel for huge pages first. The
	 * flag should be specified for processes with large heaps when huge
	 * page support is available.   
	 *
	 * See http://andrigoss.blogspot.com/2008/02/jvm-performance-tuning.html,
	 * http://java.sun.com/javase/technologies/hotspot/largememory.jsp, and
	 * 'bigdatasetup'.
	 */
	//"-XX:+UseLargePages",
	/* */
	/*
	 * Enable JXM remote management for the data service.
	 *
	 * Note: This will not work if you have two such services on a host 
	 * because it will assign the same port to each service.  In order
	 * to work around that the argument would have to be specified by
	 * the service starter and then published in the Entry[] attributes.
	 *
	 * However, you can use ssh -X to open a tunnel with X
	 * forwarding and then run jconsole locally on the target host
	 * and bring up these data services without enabling remote
	 * JMX.
	 *
	"-Dcom.sun.management.jmxremote.port=9996",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
    };

    serviceCount = bigdata.clientServiceCount;

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.cs),

	new MaxClientServicesPerHostConstraint(bigdata.maxClientServicePerHost),

    };

    properties = new NV[] {
	
    /* 
     * Option may be set to report out all metrics which can be
     * collected to the LBS.
     */
    new NV(IBigdataClient.Options.REPORT_ALL,"@REPORT_ALL@"),

    };

}

org.embergraph.service.jini.LoadBalancerServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.lbs)

    };

    args = new String[]{
    /*
     * FIXME The load balancer is a big piggish on long runs because it
     * keeps the performance counter histories in RAM.  While those histories
     * are bounded, it still uses more RAM than it should.
     */
	"-Xmx2G",
	/* Enable huge page utilization.  This is a VM and OS specific tweak. 
	 * For linux, you must configure the kernel for huge pages first. The
	 * flag should be specified for processes with large heaps when huge
	 * page support is available.   
	 *
	 * See http://andrigoss.blogspot.com/2008/02/jvm-performance-tuning.html,
	 * http://java.sun.com/javase/technologies/hotspot/largememory.jsp, and
	 * 'bigdatasetup'.
	 */
	//"-XX:+UseLargePages",
	/*
	 * Enable JXM remote management for the data service.
	 *
	 * Note: This will not work if you have two data services on a host 
	 * because it will assign the same port to each service.  In order
	 * to work around that the argument would have to be specified by
	 * the service starter and then published in the Entry[] attributes.
	 *
	"-Dcom.sun.management.jmxremote.port=9998",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
    };

    /*
     * Override some properties.
     */
    properties = new NV[] {

    /* 
     * Each JiniClient (and hence all bigdata services) can run an
     * httpd that will expose performance counters for the service and
     * the host on which it is running.  This property specifies the
     * port for that httpd service.  Valid values are port number,
     * zero (0) for a random open port, MINUS ONE (-1) to disable the
     * httpd service.
     *
     * Note: The load balancer httpd normally uses a known port so
     * that it is easy to find.  This is where you will find all of
     * the performance counters aggregated for the entire federation,
     * including their history.
     */
    new NV(IBigdataClient.Options.HTTPD_PORT, "@LOAD_BALANCER_PORT@"),

    /*
     * Note: The load balancer SHOULD NOT collect platform statistics
     * itself since that interfers with its ability to aggregate
     * statistics about the host on which it is running.  Instead it
     * should rely on the presence of at least one other service
     * running on the same host to report those statistics to the load
     * balancer.
     */
    new NV(IBigdataClient.Options.COLLECT_PLATFORM_STATISTICS,"false"),
		
    /* 
     * The directory where the aggregated statistics will be logged.
     * The load balancer will write snapshots of the historical
     * counters into this directory.  See LoadBalancerService javadoc
     * for configuration options which effect how frequently it will
     * log its counters and how many snapshots will be preserved.
     *
     * Note: You only need to specify this option if you want to put
     * the files into a well known location, e.g, on a shared volume.
     */
    //new NV(LoadBalancerServer.Options.LOG_DIR,"/nas/var/log/bigdata"),

    /* Option essentially turns off the load-based decision making for
     * this many minutes and substitutes a round-robin policy for
     * recommending the least utilized data services.  The main reason
     * to this is to force the initial allocation to be distributed as
     * evenly as possible across the data services in the cluster.
     */
    new NV(LoadBalancerServer.Options.INITIAL_ROUND_ROBIN_UPDATE_COUNT,"10"),

    };

}

/**
 * Configuration options for the distributed LUBM data generator.
 */
lubm {

    // The #of universities to generate. 
    // U8000   is 1.2B told triples
    // U25000  is 3.4B told triples.
    // U50000  is 6.7B told triples.
    // U100000 is ~12B told triples.
    static private univNum = 1000;//100000;

    // the kb namespace (based on the #of universities by default).
    //static private namespace = "U"+univNum+"";
    static private namespace = "U1000";

    // minimum #of data services to run.
//    static private minDataServices = bigdata.dataServiceCount; // unused

    // How long the master will wait to discover the minimum #of data
    // services that you specified (ms).
//    static private awaitDataServicesTimeout = 8000; // unused.

    /* Multiplier for the scatter effect.
     */
    static private scatterFactor = 2;
    static private scatterFactor_term2id = 1;

    /* The #of index partitions to allocate on a scatter split.  ZERO
     * (0) means that 2 index partitions will be allocated per
     * data service which partiticpates in the scatter split.
     * Non-zero values directly give the #of index partitions to
     * create.  
     */
    static private scatterSplitIndexPartitionCount = ConfigMath.multiply
	( scatterFactor,
	  bigdata.dataServiceCount
	  );
    static private scatterSplitIndexPartitionCount_term2id = ConfigMath.multiply
	( scatterFactor_term2id,
	  bigdata.dataServiceCount
	  );

    // Use all discovered data services when scattering an index.
    static private scatterSplitDataServiceCount = 0;

    /* Scatter split trigger point.  The scatter split will not be
     * triggered until the initial index partition has reached
     * this percentage of a nominal index partition in size.
     */
    static private scatterSplitPercentOfSplitThreshold = 0.5;//was .5

	/* 
	 * Multipliers that compensate for the consumer/producer ratio for
	 * the asynchronous index write API.  These are empirical factors
	 * based on observing the ratio (chunkWritingTime/chunkWaitingTime).
	 * Assuming a constant chunk writing time, if the chunk size for each
	 * index is adjusted by its multiplier then this ratio would be 1:1.
	 * In practice, the chunk writing time is not a linear function of
	 * the chunk size, which is one reason why we prefer larger chunks
	 * and why the asynchronous write API is a win.
	 *
	 * Note: These factors were set relative to TERM2ID.  However, when
	 * I reduced the scatterFactor for TERM2ID by 1/2, I doubled its
	 * chunk size to keep up the same throughput so it is now at 2.00
	 * rather than 1.00.
	 */
	static private chunkSizeFactor_id2term =  1.79;
	static private chunkSizeFactor_term2id =  2.00;
	static private chunkSizeFactor_spo     =  8.00; // was 3.89
	static private chunkSizeFactor_pos     =  8.00; // was 13.37
	static private chunkSizeFactor_osp     =  8.00; // was 27.35
	
	/* The nominal sink chunk size.  For each index, this is adjusted
	 * by the factor specified above.
	 */
	static private sinkChunkSize = 10000;
	 
    /*
     * Specify / override some triple store properties.
     *
     * Note: You must reference this object in the section for the
     * component which will actually create the KB instance, e.g.,
     * either the RDFDataLoadMaster or the LubmGeneratorMaster.
     */
    static private properties = new NV[] {
	
        /*
         * When "true", the store will perform incremental closure as
         * the data are loaded. When "false", the closure will be
         * computed after all data are loaded. (Actually, since we are
         * not loading through the SAIL making this true does not
         * cause incremental TM but it does disable closure, so
         * "false" is what you need here).
         */
        new NV(BigdataSail.Options.TRUTH_MAINTENANCE, "false" ),

        /*
         * May be used to turn off inference during query, but will
	 * cause ALL inferences to be filtered out when reading on the
	 * database.
         */
        // new NV(BigdataSail.Options.INCLUDE_INFERRED, "false"),

        /*
         * May be used to turn off query-time expansion of entailments such as
         * (x rdf:type rdfs:Resource) and owl:sameAs even through those
         * entailments were not materialized during forward closure (this
	 * disables the backchainer!)
         */
        new NV(BigdataSail.Options.QUERY_TIME_EXPANDER, "false"),

        /*
         * Option to restrict ourselves to RDFS only inference. This
         * condition may be compared readily to many other stores.
         * 
         * Note: While we can turn on some kinds of owl processing
         * (e.g., TransitiveProperty, see below), we can not compute
         * all the necessary entailments (only queries 11 and 13
         * benefit).
         * 
         * Note: There are no owl:sameAs assertions in LUBM.
         * 
         * Note: lubm query does not benefit from owl:inverseOf.
         * 
         * Note: lubm query does benefit from owl:TransitiveProperty
         * (queries 11 and 13).
         * 
         * Note: owl:Restriction (which we can not compute) plus
         * owl:TransitiveProperty is required to get all the answers
         * for LUBM.
         */
        new NV(BigdataSail.Options.AXIOMS_CLASS, "org.embergraph.rdf.axioms.RdfsAxioms"),
        //new NV(BigdataSail.Options.AXIOMS_CLASS,"org.embergraph.rdf.axioms.NoAxioms"),

        new NV(BigdataSail.Options.VOCABULARY_CLASS,"org.embergraph.rdf.vocab.LUBMVocabulary"),
        //new NV(BigdataSail.Options.VOCABULARY_CLASS,"org.embergraph.rdf.vocab.BSBMVocabulary"),

        /*
         * Produce a full closure (all entailments) so that the
         * backward chainer is always a NOP. Note that the
         * configuration properties are stored in the database (in the
         * global row store) so you always get exactly the same
         * configuration that you created when reopening a triple
         * store.
         */
        // new NV(BigdataSail.Options.FORWARD_CHAIN_RDF_TYPE_RDFS_RESOURCE, "true"),
        // new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_SAMEAS_PROPERTIES, "true"),

        /*
         * Additional owl inferences. LUBM only both inverseOf and
         * TransitiveProperty of those that we support (owl:sameAs,
         * owl:inverseOf, owl:TransitiveProperty), but not owl:sameAs.
         */
        new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_INVERSE_OF, "true"),
        new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_TRANSITIVE_PROPERTY, "true"),

        // Note: FastClosure is the default.
        // new NV(BigdataSail.Options.CLOSURE_CLASS, "org.embergraph.rdf.rules.FullClosure"),

        /*
         * Various things that effect native rule execution.
         */
        // new NV(BigdataSail.Options.FORCE_SERIAL_EXECUTION, "false"),
        // new NV(BigdataSail.Options.BUFFER_CAPACITY, "20000"),
        // Note: max buffer size := chunkOfChunks*chunkCapacity LT 1M (!!!)
        new NV(BigdataSail.Options.CHUNK_OF_CHUNKS_CAPACITY, "100"), // default 1000
        new NV(BigdataSail.Options.CHUNK_CAPACITY, "1000"), // default 100
        // new NV(BigdataSail.QUERY_BUFFER_CAPACITY, "10000"),
        // new NV(BigdataSail.FULLY_BUFFERED_READ_THRESHOLD, "10000"),

        /*
         * Turn off incremental closure in the DataLoader object.
         */
        new NV(org.embergraph.rdf.store.DataLoader.Options.CLOSURE, "None"),
	//org.embergraph.rdf.store.DataLoader.ClosureEnum.None.toString()),

        /*
         * Turn off commit in the DataLoader object. We do not need to commit
         * anything until we have loaded all the data and computed the closure
         * over the database.
         */
        new NV(org.embergraph.rdf.store.DataLoader.Options.COMMIT,"None"),
	//org.embergraph.rdf.store.DataLoader.CommitEnum.None.toString()),

        /*
         * Provide Unicode support for keys with locale-based string
         * collation. This is more expensive in key creation during
         * loading, but allows key comparison and sorting in the
         * specified locale in queries.
	 *
	 * @see org.embergraph.btree.keys.CollatorEnum
         */
        new NV(KeyBuilder.Options.COLLATOR,"ICU"),
        new NV(KeyBuilder.Options.USER_LANGUAGE,"en"),
        new NV(KeyBuilder.Options.USER_COUNTRY,"US"),
        new NV(KeyBuilder.Options.USER_VARIANT,""),

        /*
         * Turn off the full text index (search for literals by keyword).
         */
	new NV(BigdataSail.Options.TEXT_INDEX, "false"),

        /*
         * Turn on bloom filter for the SPO index (good up to ~2M
         * index entries for scale-up -or- for any size index for
         * scale-out).  This is a big win for some queries on
         * scale-out indices since we can avoid touching the disk if
         * the bloom filter reports "false" for a key.
         */
        new NV(BigdataSail.Options.BLOOM_FILTER, "true"),

        /* The #of low order bits from the TERM2ID index partition
         * local counter that will be reversed and written into the
         * high-order bits of the term identifier.  This has a strong
         * effect on the distribution of bulk index read/write
         * operations for the triple store.  For a given value of N, a
         * bulk write will tend to touch 2^N index partitions.
         * Therefore if this is is even roughly on the order of the number
         * of index partitions, each bulk write will tend to be scattered
         * to all index partitions.
         *
         * Note: If this value is too large then the writes WITHIN the
         * index partitions will become uniformly distributed, which
         * will negatively impact index performance.
         */
        new NV(BigdataSail.Options.TERMID_BITS_TO_REVERSE,"6"),//was 8, was 6

        /*
         * Turn off statement identifiers (support for statements
         * about statements).
         */
        new NV(BigdataSail.Options.STATEMENT_IDENTIFIERS, "false"),

        /*
	 * Option may be enabled to store blank nodes such that they
	 * are stable (they are not stored by default).
         */
        // new NV(BigdataSail.Options.STORE_BLANK_NODES,"true");

        /*
         * Turn off justification chains.  This impacts only the load
         * performance, but it is a big impact and only required if
         * you will be doing truth maintenance (TM). Also, truth
         * maintenance based on the justification chains does not
         * scale-out.  (We are planning a magic sets integration to
         * take care of that).
         */
        new NV(BigdataSail.Options.JUSTIFY, "false"),

	new NV(org.embergraph.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.WRITE_RETENTION_QUEUE_CAPACITY
		 ), "500"),// was 8000; default 500

    new NV(org.embergraph.config.Configuration.getOverrideProperty
           ( namespace,
         IndexMetadata.Options.BTREE_BRANCHING_FACTOR
         ), "32"),// default 64

	/*
	 * Turn on direct buffering for index segment nodes.
	 */
	new NV( org.embergraph.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.INDEX_SEGMENT_BUFFER_NODES
		  ), "true"),

	new NV(org.embergraph.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.INDEX_SEGMENT_BRANCHING_FACTOR
		 ), "512"),// default 512 (was 512)

	/*
	 * Tweak the asynchronous write API parameters.
	 */

	// Dial down the master/sink queue capacities to rein in RAM.
	// default is 5k.  The sink is the big RAM consumer since there
	// is one since per index partition (and one thread per sink).
	// Note: A cluster with 32-bit JVMs needs to constrain the size
	// of the sink queues or it will run out of memory.
	new NV( org.embergraph.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.MASTER_QUEUE_CAPACITY
		  ), "5000"), // was 5; was 1000; was 5000
	// default is 5k
	new NV( org.embergraph.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SINK_QUEUE_CAPACITY
		  ), "500"), // was 50; was 500, was 5000
 
	// increase the poll duration for the sinks to reduce CPU demand.
	new NV( org.embergraph.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SINK_POLL_TIMEOUT_NANOS
		  ), ""+ConfigMath.ms2ns(2000)),

 	// Override master/sink queue capacity for TERM2ID since uses KVOLatch
 	// and requires all results to be in before writes on the other indices
 	// may proceed.
// 	new NV(org.embergraph.config.Configuration.getOverrideProperty
// 	       ( namespace + "."
// 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
// 				    + LexiconKeyOrder.TERM2ID,
// 		 IndexMetadata.Options.MASTER_QUEUE_CAPACITY
// 		 ), "1000"),// default 5000
// 	new NV(org.embergraph.config.Configuration.getOverrideProperty
// 	       ( namespace + "."
// 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
// 				    + LexiconKeyOrder.TERM2ID,
// 		 IndexMetadata.Options.SINK_QUEUE_CAPACITY
//		 ), "1000"),// default 5000

	// default is 10k (global adjustment)
	new NV( org.embergraph.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SINK_CHUNK_SIZE
		  ), ""+sinkChunkSize), // was 20000

	/* per index adjustment. */
 	new NV(org.embergraph.config.Configuration.getOverrideProperty
 	       ( namespace + "."
 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
 				    + LexiconKeyOrder.TERM2ID,
 		 IndexMetadata.Options.SINK_CHUNK_SIZE
		 ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_term2id)),
 	new NV(org.embergraph.config.Configuration.getOverrideProperty
 	       ( namespace + "."
 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
 				    + LexiconKeyOrder.ID2TERM,
 		 IndexMetadata.Options.SINK_CHUNK_SIZE
		 ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_id2term)),
    new NV(org.embergraph.config.Configuration.getOverrideProperty
           ( namespace + "."    + SPORelation.NAME_SPO_RELATION + "."
                                + SPOKeyOrder.SPO,
             IndexMetadata.Options.SINK_CHUNK_SIZE
             ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_spo)),
    new NV(org.embergraph.config.Configuration.getOverrideProperty
       ( namespace + "."        + SPORelation.NAME_SPO_RELATION + "."
                                + SPOKeyOrder.POS,
             IndexMetadata.Options.SINK_CHUNK_SIZE
             ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_pos)),
    new NV(org.embergraph.config.Configuration.getOverrideProperty
           ( namespace + "."    + SPORelation.NAME_SPO_RELATION + "."
                                + SPOKeyOrder.OSP,
             IndexMetadata.Options.SINK_CHUNK_SIZE
             ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_osp)),

// 	// default is infinite
// 	new NV( org.embergraph.config.Configuration.getOverrideProperty
// 		( namespace,
// 		  IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
// 		  ), ""+Long.MAX_VALUE),//ConfigMath.s2ns(120)),

 	/* The default is infinite.  A smaller value is necessary to close down sinks
 	 * on which the client never writes but which are not closed by a redirect. 
 	 * The idle timeout will cause sinks which are not receiving ANY chunks to
 	 * close themselves.  Sinks which are receiving chunks but are not filling
 	 * their target chunk size in time will be flushed when the chunk timeout is
 	 * exceeded.
 	 */
 	new NV( org.embergraph.config.Configuration.getOverrideProperty
 		( namespace,
 		  IndexMetadata.Options.SINK_IDLE_TIMEOUT_NANOS
		  ), ""+ConfigMath.s2ns(60)),

	/* The TERM2ID index uses idle timeout to preserve liveness for async writes.
	 * This is required in order for the async loader to not wait forever for the
	 * last full chunk when bulk loading with async writes on the TERM2ID index.
	 * The value of this timeout effects both how large the chunks will be and
	 * how long after the last document is parsed it will be until the TERM2ID
	 * async write buffers decide that they are idle and evict the last of their
	 * chunks.  This also effects the throughput since the larger chunk sizes
	 * correlates very well with higher throughput.
	 */
         new NV(org.embergraph.config.Configuration.getOverrideProperty
 	       ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION + "." + LexiconKeyOrder.TERM2ID,
 	         IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(20)),

	/* The following timeouts were derived from performance after 2 hours
	 * based on U100000b/run4.  TPS was 183k at that point in run4.  I introduced
	 * the idle timeout for these indices in U100000b/run7 since clients were
	 * slowing down as the #of index partitions increased over time since they
	 * have to fill a chunk before writing on the corresponding index partition
	 * but the #of index partitions was increasing, forcing the clients to 
	 * process more documents before driving a write on any given index partition.
	 * I chose a value at 2h since the queue length on the data services will
	 * increase over time as more index partitions are created and the clients
	 * submit more tasks which are queued concurrenty.  This is just a heuristic.
	 * A different mechansim should be used to increase client load in proportion
	 * with the database scale.
	 */
         new NV(org.embergraph.config.Configuration.getOverrideProperty
 	       ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION + "." + LexiconKeyOrder.ID2TERM,
 	         IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(95)),
         new NV(org.embergraph.config.Configuration.getOverrideProperty
 	       ( namespace + "." + SPORelation.NAME_SPO_RELATION + "." + SPOKeyOrder.SPO,
 	         IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(130)),
         new NV(org.embergraph.config.Configuration.getOverrideProperty
 	       ( namespace + "." + SPORelation.NAME_SPO_RELATION + "." + SPOKeyOrder.OSP,
 	         IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(130)),
         new NV(org.embergraph.config.Configuration.getOverrideProperty
 	       ( namespace + "." + SPORelation.NAME_SPO_RELATION + "." + SPOKeyOrder.POS,
 	         IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(110)),


	// scatter split overrides


	new NV( org.embergraph.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SCATTER_SPLIT_PERCENT_OF_SPLIT_THRESHOLD
		  ), ""+scatterSplitPercentOfSplitThreshold),

	new NV( org.embergraph.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SCATTER_SPLIT_DATA_SERVICE_COUNT
		  ), ""+scatterSplitDataServiceCount),

	new NV( org.embergraph.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SCATTER_SPLIT_INDEX_PARTITION_COUNT
		  ), ""+scatterSplitIndexPartitionCount),
    new NV(org.embergraph.config.Configuration
       .getOverrideProperty(namespace + "."
			    + LexiconRelation.NAME_LEXICON_RELATION + "."
			    + LexiconKeyOrder.TERM2ID,
			    IndexMetadata.Options.SCATTER_SPLIT_INDEX_PARTITION_COUNT),
       "" +scatterSplitIndexPartitionCount_term2id),

    };

}

/**
 * Mapped, distributed bulk loader configuration.
 */
org.embergraph.rdf.load.MappedRDFDataLoadMaster {

    // The job name (same as the KB namespace by default).
    jobName = lubm.namespace;

	// The #of client tasks to execute.
    nclients = bigdata.clientServiceCount;

    /* The directory for scheduled index partition dumps for runs. May be NAS
     * or a local file system.  Only the master writes on this directory.
     */
	indexDumpDir = new File("@NAS@/"+jobName+"-indexDumps");

	// must be specified for indexDumpDir to work.
	indexDumpNamespace = lubm.namespace;
	
    // The KB name.
    namespace = lubm.namespace;
    
	// KB properties made visible to JiniFederation#getProperties()
	properties = lubm.properties;

    // Scanner identifies resources to be loaded.
    resourceScannerFactory = org.embergraph.service.jini.master.FileSystemScanner.newFactory(
    	//new File("/global/data/bsbm3/bsbm3_200m_1MSplits"), // dataDir
    	new File("/global/data/lubm/U1000"), // dataDir
    	new org.embergraph.rdf.load.RDFFilenameFilter() // optional filename filter.
    	);

    // The ontology to load (file or directory) when the KB is created.
    // This is a directory containing the ontology and some pre-generated data sets.
    //ontology = new File("/global/data/emptyDir");
    ontology = new File("/global/data/univ-bench.owl");
    // This is the directory into which the ontology is installed by 'ant lubm-install'.
    //ontology = new File("/nas/bigdata/benchmark/lubm/config/univ-bench.owl");

	// The maximum thread pool size for RDF parser tasks.
    //parserPoolSize = 5;

	// The capacity of the work queue for the parser thread pool.
	//parserQueueCapacity = parserPoolSize;

	// The maximum #of parsed but not yet buffered statements before the parser
	// thread pool is paused (limits RAM demand by the client).
	unbufferedStatementThreshold = ConfigMath.multiply(1000,Bytes.kilobyte);

	// The maximum thread pool size for buffering writes for the TERM2ID index.
	//term2IdWriterPoolSize = 5;
	
	// The maximum thread pool size for buffering writes for the TERM2ID index.
	//otherWriterPoolSize = 5;

	// The maximum thread pool size asynchronous notifications.
	// notifyPoolSize = 5;

	// Size of chunks written onto the asynchronous write API buffers.
	producerChunkSize = 20000;
	
	// Initial capacity of the RDF Value hash map.
	valuesInitialCapacity = 20000;
	
	// Initial capacity of the RDF Blank node hash map.
	bnodesInitialCapacity = 16;

    // create the KB if not found.
    create = true;

    // when true, deletes each source file once loaded successfully.
    deleteAfter = false;

    // when true, loads data.
    loadData = true;

    // when true, computes closure.
    computeClosure = true;

	/*
	 * When true, requests a compacting merge of the data services in
	 * the federation before computing the closure.
	 */
	forceOverflowBeforeClosure = false;

	// force overflow of all data services after the job ends (optimizes for query).
	forceOverflow = false;

    /* How long the master will wait in milliseconds to discover the services
     * that you specify for [servicesTemplates] and [clientsTemplate].
     */
    awaitServicesTimeout = 10000;
    
	/* The minimum set of services which must be discovered before the master
	 * can start.
	 */
	servicesTemplates = new ServicesTemplate[] {

		// the metadata service
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					org.embergraph.service.IMetadataService.class
				},
				null/*attributes*/),
			null/*filter*/
			),

		// the data services (filter is required to exclude metadata services)
		new ServicesTemplate(
			bigdata.dataServiceCount/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					org.embergraph.service.IDataService.class
				},
				null/*attributes*/),
			DataServiceFilter.INSTANCE/*filter*/
			),

		// the load balancer
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[] {
					org.embergraph.service.ILoadBalancerService.class
				},
				null/*attributes*/),
			null/*filter*/
			)
		
	};

    /* Template for matching the services to which the clients will be
     * distributed for execution.  Normally you will specify
     * IClientService as the interface to be discovered.  While it is
     * possible to run tasks on an IDataService or even an
     * IMetadataService since they both implement IRemoteExecutor, it
     * is generally discouraged unless the tasks require explicit
     * access to the local index partitions for their execution.
     */
	clientsTemplate = new ServicesTemplate(
			bigdata.clientServiceCount, // minMatches
			new ServiceTemplate(
				null, //serviceID
				new Class[]{
					org.embergraph.service.IClientService.class
				},
				null // attributes
			    ),
			null // filter
			);

	/*
	 * RDF distributed data loader options.
	 */

}

org.embergraph.service.jini.util.DumpFederation {

    discoveryDelay = 5000; // ms

    namespace = lubm.namespace;

}

org.embergraph.service.jini.util.BroadcastSighup {

    discoveryDelay = 5000; // ms

    pushConfig = false;
    
    restartServices = true;

}

org.embergraph.service.jini.util.ListServices {

   discoveryDelay = 5000; // ms
				   
   showServiceItems = false;

}
