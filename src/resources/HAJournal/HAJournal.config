import net.jini.jeri.BasicILFactory;
import net.jini.jeri.BasicJeriExporter;
import net.jini.jeri.tcp.TcpServerEndpoint;

import net.jini.discovery.LookupDiscovery;
import net.jini.core.discovery.LookupLocator;
import net.jini.core.entry.Entry;
import net.jini.lookup.entry.Name;
import net.jini.lookup.entry.Comment;
import net.jini.lookup.entry.Address;
import net.jini.lookup.entry.Location;
import net.jini.lookup.entry.ServiceInfo;
import net.jini.core.lookup.ServiceTemplate;

import java.io.File;
import java.net.InetAddress;
import java.net.InetSocketAddress;
import java.util.UUID;

import com.bigdata.util.NV;
import com.bigdata.util.config.NicUtil;
import com.bigdata.journal.Options;
import com.bigdata.journal.BufferMode;
import com.bigdata.journal.Journal;
import com.bigdata.journal.jini.ha.*;
import com.bigdata.jini.lookup.entry.*;
import com.bigdata.service.IBigdataClient;
import com.bigdata.service.AbstractTransactionService;
import com.bigdata.service.jini.*;
import com.bigdata.service.jini.lookup.DataServiceFilter;
import com.bigdata.service.jini.master.ServicesTemplate;
import com.bigdata.jini.start.config.*;
import com.bigdata.jini.util.ConfigMath;

import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.data.ACL;
import org.apache.zookeeper.data.Id;

// imports for various options.
import com.bigdata.btree.IndexMetadata;
import com.bigdata.btree.keys.KeyBuilder;
import com.bigdata.rdf.sail.BigdataSail;
import com.bigdata.rdf.spo.SPORelation;
import com.bigdata.rdf.spo.SPOKeyOrder;
import com.bigdata.rdf.lexicon.LexiconRelation;
import com.bigdata.rdf.lexicon.LexiconKeyOrder;
import com.bigdata.rawstore.Bytes;

/*
 * This is a sample configuration file for a highly available Journal. A
 * version of this file must be available to each HAJournalServer in the
 * pipeline.
 */

/*
 * Globals.
 */
bigdata {

   /* The name of the federation (also constrains the discovery groups and 
    * provides a zk namespace). This can be overridden from the environment.
    */
   private static fedname = ConfigMath.getProperty("FEDNAME","benchmark");

   // The RMI port for the HAGlue interface (may be ZERO for a random port).
   private static rmiPort = Integer.parseInt(ConfigMath.getProperty("RMI_PORT","9080"));

   // write replication pipeline port (listener).
   private static haPort = Integer.parseInt(ConfigMath.getProperty("HA_PORT","9090"));
   
   // The #of services in the write pipeline.
   private static replicationFactor = Integer.parseInt(ConfigMath.getProperty("REPLICATION_FACTOR","3"));

   // The logical service identifier shared by all members of the quorum.
   private static logicalServiceId = ConfigMath.getProperty("LOGICAL_SERVICE_ID","HAJournal-1");
   
   // The ServiceID for *this* service -or- null to assign it dynamically.
   private static serviceId = null;
   
   // The base directory for the federation.
   private static fedDir = new File(ConfigMath.getProperty("FED_DIR","."),fedname);

   // The service directory (if serviceId is null, then you must override).
   // private static serviceDir = new File(fedname,""+serviceId);
   //private static serviceDir = new File(fedname,logicalServiceId+File.separator+"HAJournalServer");
   private static serviceDir = new File(fedDir,logicalServiceId+File.separator+"HAJournalServer");
   
   // journal data directory.
   private static dataDir = new File(ConfigMath.getProperty("DATA_DIR",""+serviceDir));

   // HA log directory.
   private static haLogDir = new File(ConfigMath.getProperty("HALOG_DIR",""+serviceDir+File.separator+"HALog"));
   //private static haLogDir = new File(serviceDir,"HALog");
   
   // Snapshot directory.
   private static snapshotDir = new File(ConfigMath.getProperty("SNAPSHOT_DIR",""+serviceDir+File.separator+"snapshot"));
   //private static snapshotDir = new File(serviceDir,"snapshot");

   /* Snapshot policy.  Choose one.
    *
    * NoSnapshotPolicy - use this if you schedule snapshots yourself, e.g.,
    * from a cron job using "GET .../status?snapshot" to request a snapshot.
    * You can specify an optional percentage as ?snapshot=hhmm just as with
    * the DefaultSnapshotPolicy.
    *
    * DefaultSnapshotPolicy(200,20) - automatic daily snapshots @ 0200 if 
    * HALogs >= 20% of Journal on disk. Arguments are when to evaluate the
    * policy as an integer (hhmm) and the percentage threshold to take a 
    * snapshot (in 0:100).
    */
   // private static snapshotPolicy = new NoSnapshotPolicy(); 
   private static snapshotPolicy = new DefaultSnapshotPolicy(200/*hhmm*/,20/*percent*/);
   
   // Restore policy
   private static restorePolicy = new DefaultRestorePolicy(ConfigMath.d2ms(7));
   
   // one federation, multicast discovery.
   //static private groups = LookupDiscovery.ALL_GROUPS;

   // unicast discovery or multiple setups, MUST specify groups.
   static private groups = ConfigMath.getGroups(ConfigMath.getProperty("GROUPS",bigdata.fedname));

    /**
     * One or more unicast URIs of the form <code>jini://host/</code>
     * or <code>jini://host:port/</code> (no default).
     *
     * This MAY be an empty array if you want to use multicast
     * discovery <strong>and</strong> you have specified the groups as
     * LookupDiscovery.ALL_GROUPS (a <code>null</code>).
     */
    static private locators = ConfigMath.getLocators(ConfigMath.getProperty("LOCATORS","jini://bigdata15/,jini://bigdata16/,jini://bigdata17/"));
    
    /**
     * A common point to set the Zookeeper client's requested
     * sessionTimeout and the jini lease timeout.  The default lease
     * renewal period for jini is 5 minutes while for zookeeper it is
     * more like 5 seconds.  This puts the two systems onto a similar
     * timeout period so that a disconnected client is more likely to
     * be noticed in roughly the same period of time for either
     * system.  A value larger than the zookeeper default helps to
     * prevent client disconnects under sustained heavy load.
     *
     * If you use a short lease timeout (LT 20s), then you need to override 
     * properties properties for the net.jini.lease.LeaseRenewalManager
     * or it will run in a tight loop (it's default roundTripTime is 10s
     * and it schedules lease renewals proactively.)
     */

    // jini
    static private leaseTimeout = ConfigMath.s2ms(60); // 20

    // zookeeper
    static private sessionTimeout = (int)ConfigMath.s2ms(60); // 5 

    /*
     * Configuration for default KB.
     */

    private static namespace = "kb";
    
    private static kb = new NV[] {
      
      /* Setup for QUADS mode without the full text index. 
      new NV(BigdataSail.Options.TRUTH_MAINTENANCE, "false" ),
      new NV(BigdataSail.Options.QUADS, "true"),
      new NV(BigdataSail.Options.STATEMENT_IDENTIFIERS, "false"),
      new NV(BigdataSail.Options.TEXT_INDEX, "false"),
      new NV(BigdataSail.Options.AXIOMS_CLASS,"com.bigdata.rdf.axioms.NoAxioms"),
      */
      /* Setup for triples without inference or the full text index. */
      new NV(BigdataSail.Options.TRUTH_MAINTENANCE, "false" ),
      new NV(BigdataSail.Options.QUADS, "false"),
      new NV(BigdataSail.Options.STATEMENT_IDENTIFIERS, "false"),
      new NV(BigdataSail.Options.TEXT_INDEX, "false"),
      new NV(BigdataSail.Options.AXIOMS_CLASS,"com.bigdata.rdf.axioms.NoAxioms"),
      
      new NV(BigdataSail.Options.QUERY_TIME_EXPANDER, "false"),

      // Bump up the branching factor for the lexicon indices on the named kb.
      // com.bigdata.namespace.kb.lex.com.bigdata.btree.BTree.branchingFactor=400
      new NV(com.bigdata.config.Configuration.getOverrideProperty
          ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION,
            IndexMetadata.Options.BTREE_BRANCHING_FACTOR
            ), "400"),

      // Bump up the branching factor for the statement indices on the named kb.
      // com.bigdata.namespace.kb.spo.com.bigdata.btree.BTree.branchingFactor=1024
      new NV(com.bigdata.config.Configuration.getOverrideProperty
          ( namespace + "." + SPORelation.NAME_SPO_RELATION,
            IndexMetadata.Options.BTREE_BRANCHING_FACTOR
            ), "1024"),

/* BSBM 100M optimizations.

      new NV(com.bigdata.config.Configuration.getOverrideProperty
          ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION+".TERM2ID",
            IndexMetadata.Options.BTREE_BRANCHING_FACTOR
            ), "300"),

      new NV(com.bigdata.config.Configuration.getOverrideProperty
          ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION+".ID2TERM",
            IndexMetadata.Options.BTREE_BRANCHING_FACTOR
            ), "800"),

      new NV(com.bigdata.config.Configuration.getOverrideProperty
          ( namespace + "." + SPORelation.NAME_SPO_RELATION+".OSP",
            IndexMetadata.Options.BTREE_BRANCHING_FACTOR
            ), "800"),

      // Tighter coding for the BSBM vocabulary.
      new NV(BigdataSail.Options.VOCABULARY_CLASS,"com.bigdata.rdf.vocab.BSBMVocabulary"),
      
      // Inlining for "USD" datatype.
      new NV(BigdataSail.Options.EXTENSION_FACTORY_CLASS,"com.bigdata.rdf.internal.BSBMExtensionFactory"),

    // Override the number of statements for batch inserts (default 10k)
    new NV(com.bigdata.rdf.sail.BigdataSail.Options.BUFFER_CAPACITY,"1000000"),

*/

    };

}

/*
 * Zookeeper client configuration.
 */
org.apache.zookeeper.ZooKeeper {

    /* Root znode for the federation instance. */
    zroot = "/" + bigdata.fedname;

    /* A comma separated list of host:port pairs, where the port is
     * the CLIENT port for the zookeeper server instance.
     */
    // ensemble
    servers = ConfigMath.getProperty("ZK_SERVERS","bigdata15:2081,bigdata16:2081,bigdata17:2081");

    /* Session timeout (optional). */
    sessionTimeout = bigdata.sessionTimeout;

    /* 
     * ACL for the zookeeper nodes created by the bigdata federation.
     *
     * Note: zookeeper ACLs are not transmitted over secure channels
     * and are placed into plain text Configuration files by the
     * ServicesManagerServer.
     */
    acl = new ACL[] {

       new ACL(ZooDefs.Perms.ALL, new Id("world", "anyone"))

    };

}

/*
 * You should not have to edit below this line.
 */

/*
 * Jini client configuration.
 */
com.bigdata.service.jini.JiniClient {

    groups = bigdata.groups;

    locators = bigdata.locators;
    
    entries = new Entry[] {
       
       // Optional metadata entries.
           
    };

}

net.jini.lookup.JoinManager {

   maxLeaseDuration = bigdata.leaseTimeout;
   
}

/*
 * HAClient configuration options.
 */
com.bigdata.journal.jini.ha.HAClient {

   cacheMissTimeout = 2000; // ms

}

/*
 * Server configuration options.
 */
com.bigdata.journal.jini.ha.HAJournalServer {

   serviceDir = bigdata.serviceDir;

   logicalServiceId = bigdata.logicalServiceId;

   writePipelineAddr = new InetSocketAddress(//
                    InetAddress.getByName(//
                            NicUtil.getIpAddress("default.nic", "default",
                                    false// loopbackOk
                            )), //
                    bigdata.haPort
            );

   replicationFactor = bigdata.replicationFactor;

   exporter = new BasicJeriExporter(TcpServerEndpoint.getInstance(bigdata.rmiPort),
                            new BasicILFactory());

   haLogDir = bigdata.haLogDir;

   snapshotDir = bigdata.snapshotDir;

   snapshotPolicy = bigdata.snapshotPolicy;
   
   restorePolicy = bigdata.restorePolicy;
   
}

/*
 * Journal configuration.
 */
com.bigdata.journal.jini.ha.HAJournal {

   properties = (NV[]) ConfigMath.concat(new NV[] {
   
      new NV(Options.FILE,
         ConfigMath.getAbsolutePath(new File(bigdata.dataDir,"bigdata-ha.jnl"))),
   
      new NV(Options.BUFFER_MODE,""+BufferMode.DiskRW),
	  // Enable group commit. See http://wiki.blazegraph.com/wiki/index.php/GroupCommit and BLZG-192.
      new NV(com.bigdata.journal.Journal.Options.GROUP_COMMIT,System.getProperty("groupCommit","false")),

      new NV(Options.WRITE_CACHE_BUFFER_COUNT,ConfigMath.getProperty("WRITE_CACHE_BUFFER_COUNT","2000")),

      new NV(IndexMetadata.Options.WRITE_RETENTION_QUEUE_CAPACITY,"4000"),

      new NV(IndexMetadata.Options.BTREE_BRANCHING_FACTOR,"128"),

      new NV(AbstractTransactionService.Options.MIN_RELEASE_AGE,"1"),

      /* Enable statistics collection and reporting.
       *
       * Note: Some of these options have external dependencies, such as
       * sysstat (pidstat, iostat) for OS and process level performance
       * counter or ganglia (to aggregate and view collected performance
       * data).
       */

      // performance counters for internal queues.
      new NV(Journal.Options.COLLECT_QUEUE_STATISTICS,
         ConfigMath.getProperty("COLLECT_QUEUE_STATISTICS","false")),
      
      // Platform and process performance counters. This requires external
      // software on some platforms (vmstat, pidstat, iostat, etc.). 
      //
      // This is necessary for the GangliaLBSPolicy or CountersLBSPolicy.
      new NV(Journal.Options.COLLECT_PLATFORM_STATISTICS,
         ConfigMath.getProperty("COLLECT_PLATFORM_STATISTICS","false")),
         
      // Use embergraph-ganglia module to build internal model of cluster load.
      //
      // This is required for the GangliaLBSPolicy.
      new NV(com.bigdata.journal.GangliaPlugIn.Options.GANGLIA_LISTEN,
         ConfigMath.getProperty("GANGLIA_LISTEN","false")),

      // Use embergraph-ganglia module to report service metrics to ganglia.
      //
      // This MAY be used INSTEAD of installing gmond.
      new NV(com.bigdata.journal.GangliaPlugIn.Options.GANGLIA_REPORT,
         ConfigMath.getProperty("GANGLIA_REPORT","false")),
      
   }, bigdata.kb);

}
